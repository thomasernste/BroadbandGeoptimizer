{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|            dba_name|   census_block|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+--------------------+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|   unWired Broadband|060770053034016|   CA|        70|          21|         5|         200|       200|\n",
      "|                  t6|550590029041059|   WI|        70|           5|         1|        1000|      1000|\n",
      "|   Veracity Networks|490351031002010|   UT|        30|          12|        12|          12|        12|\n",
      "|Monmouth Telephon...|340155016082005|   NJ|        50|           0|         0|         100|       100|\n",
      "|Morris Broadband,...|371139706003009|   NC|        42|         100|         5|         100|         5|\n",
      "|                null|           null| null|      null|        null|      null|        null|      null|\n",
      "|Great Basin Inter...|320310012012053|   NV|        70|          12|         3|           0|         0|\n",
      "|                  t6|170759506001009|   IL|        70|           5|         1|        1000|      1000|\n",
      "|               Digis|490451307011026|   UT|        70|          15|         3|        1000|      1000|\n",
      "| StarTouch Broadband|530770018004053|   WA|        70|          25|        25|        1000|      1000|\n",
      "|                  t6|551270016031002|   WI|        70|           5|         1|        1000|      1000|\n",
      "|          Centennial|720757108004017|   PR|        70|           0|         0|           7|         7|\n",
      "|Monmouth Telephon...|340390331005006|   NJ|        50|           0|         0|         100|       100|\n",
      "|   Veracity Networks|490490028021011|   UT|        50|        1000|      1000|        1000|      1000|\n",
      "|                null|           null| null|      null|        null|      null|        null|      null|\n",
      "|          Centennial|721191304021003|   PR|        70|           0|         0|        1000|      1000|\n",
      "|South Central UT ...|490011002001316|   UT|        12|          50|         3|          50|         3|\n",
      "|                null|           null| null|      null|        null|      null|        null|      null|\n",
      "|   Polar Telcom, INC|380999583002432|   ND|        70|           5|         1|           0|         0|\n",
      "|Morris Broadband,...|370899307031076|   NC|        42|         100|         5|         100|         5|\n",
      "+--------------------+---------------+-----+----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This script processes a dataset available in an S3 bucket conataining information about broadband availability across the United States.            # Specifically, the data shows available broadband speeds,the broadband         # infrastrucure technologies, and the provider names for broadband systems      # across the more than 11 million US census blocks in the United States.\n",
    "# The data source, file structure including variable names, and other documentation for the broadband dataset is available here: \n",
    "# https://opendata.fcc.gov/Wireline/Fixed-Broadband-Deployment-Data-June-2017-Status-V/9r8r-g7ut\n",
    "\n",
    "# If PYTHONPATH is not set, findspark and findspark.init() will find it on your machine \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import spark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "\n",
    "\n",
    "# schemaString = 'something'\n",
    "\n",
    "# fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local') \\\n",
    "        .appName(\"BroadbandScout\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "pathname_input = 's3a://sparkforinsightproject/Fixed_Broadband_Deployment_Data__June__2017_Status_V1.csv'\n",
    "\n",
    "pathname_output = 's3a://sparkforinsightproject/database_data/sparkdf_broadband_output_2'\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def etl_broadband(input_data_txt):\n",
    "\n",
    "    broadband_schema = StructType([\n",
    "        StructField(\"Logical Record Number\", StringType(), True),\n",
    "        StructField(\"Provider ID\", StringType(), True),\n",
    "        StructField(\"FRN\", StringType(), True),\n",
    "        StructField(\"Provider Name\", StringType(), True),\n",
    "        StructField(\"DBA Name\", StringType(), True),\n",
    "        StructField(\"Holding Company Name\", StringType(), True),\n",
    "        StructField(\"Holding Company Number\", StringType(), True),\n",
    "        StructField(\"Holding Company Final\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Census Block FIPS Code\", StringType(), True),\n",
    "        StructField(\"Technology Code\", StringType(), True),\n",
    "        StructField(\"Consumer\", StringType(), True),\n",
    "        StructField(\"Max Advertised Downstream Speed (mbps)\", IntegerType(), True),\n",
    "        StructField(\"Max Advertised Upstream Speed (mbps)\", IntegerType(), True),\n",
    "        StructField(\"Business\", StringType(), True),\n",
    "        StructField(\"Max CIR Downstream Speed (mbps)\", IntegerType(), True),\n",
    "        StructField(\"Max CIR Upstream Speed (mbps)\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "    df_BROADBAND = spark.read.csv(input_data_txt, quote='\"', header=True, sep=',', nullValue='NA', schema=broadband_schema)\n",
    "\n",
    "    df_BROADBAND = df_BROADBAND\\\n",
    "                               .withColumnRenamed(\"DBA Name\", \"dba_name\")\\\n",
    "                               .withColumnRenamed(\"State\", \"state\")\\\n",
    "                               .withColumnRenamed(\"Census Block FIPS Code\", \"census_block\")\\\n",
    "                               .withColumnRenamed(\"Technology Code\", \"technology\")\\\n",
    "                               .withColumnRenamed(\"Max Advertised Downstream Speed (mbps)\", \"ma_downspeed\")\\\n",
    "                               .withColumnRenamed(\"Max Advertised Upstream Speed (mbps)\", \"ma_upspeed\")\\\n",
    "                               .withColumnRenamed(\"Max CIR Downstream Speed (mbps)\",  \"mc_downspeed\")\\\n",
    "                               .withColumnRenamed(\"Max CIR Upstream Speed (mbps)\",  \"mc_upspeed\")\n",
    "\n",
    "\n",
    "    # This code selects and saves just seven of the 16 columns from the \n",
    "    # original file that have some clear potential value for the database.\n",
    "\n",
    "    df_BROADBAND = df_BROADBAND\\\n",
    "                                       .select(\n",
    "                                       \"dba_name\",\\\n",
    "                                       \"census_block\",\\\n",
    "                                       \"state\",\\\n",
    "                                       \"technology\",\\\n",
    "                                       \"ma_downspeed\",\\\n",
    "                                       \"ma_upspeed\",\\\n",
    "                                       \"mc_downspeed\",\\\n",
    "                                       \"mc_upspeed\")\n",
    "\n",
    "    return df_BROADBAND\n",
    "\n",
    "# def main():\n",
    "#     input_data_txt = sys.argv[1]\n",
    "#     output_data_txt = sys.argv[2]\n",
    "#     extract_transform_load_broadband(input_data_txt, output_data_txt)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "output_df_broadband = etl_broadband(pathname_input)\n",
    "\n",
    "\n",
    "output_df_broadband.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|060770053034016|   CA|        70|          21|         5|         200|       200|\n",
      "|550590029041059|   WI|        70|           5|         1|        1000|      1000|\n",
      "|490351031002010|   UT|        30|          12|        12|          12|        12|\n",
      "|340155016082005|   NJ|        50|           0|         0|         100|       100|\n",
      "|371139706003009|   NC|        42|         100|         5|         100|         5|\n",
      "|320310012012053|   NV|        70|          12|         3|           0|         0|\n",
      "|170759506001009|   IL|        70|           5|         1|        1000|      1000|\n",
      "|490451307011026|   UT|        70|          15|         3|        1000|      1000|\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|\n",
      "|551270016031002|   WI|        70|           5|         1|        1000|      1000|\n",
      "|720757108004017|   PR|        70|           0|         0|           7|         7|\n",
      "|340390331005006|   NJ|        50|           0|         0|         100|       100|\n",
      "|490490028021011|   UT|        50|        1000|      1000|        1000|      1000|\n",
      "|721191304021003|   PR|        70|           0|         0|        1000|      1000|\n",
      "|490011002001316|   UT|        12|          50|         3|          50|         3|\n",
      "|380999583002432|   ND|        70|           5|         1|           0|         0|\n",
      "|370899307031076|   NC|        42|         100|         5|         100|         5|\n",
      "|270471803001083|   MN|        50|         100|       100|        1000|      1000|\n",
      "|170898545011028|   IL|        70|           5|         1|        1000|      1000|\n",
      "|370570617021033|   NC|        11|          10|         1|           0|         0|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count after dropping null rows\n",
    "\n",
    "df_na_dropped = output_df_broadband.na.drop()\n",
    "df_na_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_df_broadband_WA_ONLY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378559"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df_broadband_WA_ONLY.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Washington only dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330304013022|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530439604003069|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530610402002017|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730102002089|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530750009003056|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330029003005|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530330320104003|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530250109011002|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730101001095|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530479704003101|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530250114024024|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530179502001205|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330323201017|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530530729052021|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530150016002073|   WA|        12|          50|         5|          50|         5|\n",
      "|530479703001218|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530330326011048|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530559603003033|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530090009001000|   WA|        70|           0|         0|        1000|      1000|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_BROADBAND_WA_ONLY = output_df_broadband.filter(output_df_broadband[\"state\"] == \"WA\")\n",
    "    \n",
    "\n",
    "df_BROADBAND_WA_ONLY.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378559"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BROADBAND_WA_ONLY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-6faeb3a6b3d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mudf_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserDefinedFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mudf_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserDefinedFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfirst_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mudf_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanging_column\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msecond_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mudf_second\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanging_column\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msecond_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         raise ValueError(\"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n\u001b[0m\u001b[1;32m    636\u001b[0m                          \"'~' for 'not' when building DataFrame boolean expressions.\")\n\u001b[1;32m    637\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "# Change technology column to 1 or 0\n",
    "\n",
    "# from pyspark.sql.functions import UserDefinedFunction\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "# changing_column = 'technology'\n",
    "# udf_first = UserDefinedFunction(lambda x: 1, IntegerType())\n",
    "# udf_second = UserDefinedFunction(lambda x: 0, IntegerType())\n",
    "# first_df = df_BROADBAND_WA_ONLY.select(*[udf_first(changing_column) if column == 50 else column for column in df_BROADBAND_WA_ONLY])\n",
    "# second_df = first_df.select(*[udf_second(changing_column) if column != 50 else column for column in df_BROADBAND_WA_ONLY])\n",
    "# second_df.show()\n",
    "\n",
    "new_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Washington Only with null values removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "df_BROADBAND_WA_ONLY_NONULL = df_BROADBAND_WA_ONLY.filter(isnan(df_BROADBAND_WA_ONLY[\"ma_downspeed\"]))\n",
    "df_BROADBAND_WA_ONLY_NONULL.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "|census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "|           0|    0|         0|           0|         0|           0|         0|\n",
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'DataFrame' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d5f0e77ae4a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'DataFrame' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# # Count null values\n",
    "\n",
    "# from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# df_test = df_BROADBAND_WA_ONLY - df_BROADBAND_WA_ONLY.select([count(when(isnan(c), c)).alias(c) for c in df_BROADBAND_WA_ONLY.columns]).show()\n",
    "\n",
    "# df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330304013022|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530439604003069|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530610402002017|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730102002089|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530750009003056|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330029003005|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530330320104003|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530250109011002|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730101001095|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530479704003101|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530250114024024|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530179502001205|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330323201017|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530530729052021|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530150016002073|   WA|        12|          50|         5|          50|         5|\n",
      "|530479703001218|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530330326011048|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530559603003033|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530090009001000|   WA|        70|           0|         0|        1000|      1000|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "\n",
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "\n",
    "df_BROADBAND_WA_ONLY.select([to_null(c).alias(c) for c in df_BROADBAND_WA_ONLY.columns]).na.drop().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional code for filtering null values\n",
    "| df_BROADBAND_WA_ONLY[\"ma_downspeed\"].isNull() | isnan(df_BROADBAND_WA_ONLY[\"ma_downspeed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove some number of characters from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[census_tract: string, state: string, technology: string, ma_downspeed: int, ma_upspeed: int, mc_downspeed: int, mc_upspeed: int]>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_na_dropped.printSchema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|census_tract_11|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|    53077001800|\n",
      "|530330304013022|   WA|        70|           0|         0|        1000|      1000|    53033030401|\n",
      "|530439604003069|   WA|        70|          25|        25|        1000|      1000|    53043960400|\n",
      "|530610402002017|   WA|        70|          25|        25|        1000|      1000|    53061040200|\n",
      "|530730102002089|   WA|        70|          25|        25|        1000|      1000|    53073010200|\n",
      "|530750009003056|   WA|        70|          25|        25|        1000|      1000|    53075000900|\n",
      "|530330029003005|   WA|        70|           0|         0|        1000|      1000|    53033002900|\n",
      "|530330320104003|   WA|        70|           0|         0|        1000|      1000|    53033032010|\n",
      "|530250109011002|   WA|        70|          25|        25|        1000|      1000|    53025010901|\n",
      "|530730101001095|   WA|        70|          25|        25|        1000|      1000|    53073010100|\n",
      "|530479704003101|   WA|        70|           0|         0|        1000|      1000|    53047970400|\n",
      "|530250114024024|   WA|        70|          25|        25|        1000|      1000|    53025011402|\n",
      "|530179502001205|   WA|        70|          25|        25|        1000|      1000|    53017950200|\n",
      "|530330323201017|   WA|        70|           0|         0|        1000|      1000|    53033032320|\n",
      "|530530729052021|   WA|        70|           0|         0|        1000|      1000|    53053072905|\n",
      "|530150016002073|   WA|        12|          50|         5|          50|         5|    53015001600|\n",
      "|530479703001218|   WA|        70|           0|         0|        1000|      1000|    53047970300|\n",
      "|530330326011048|   WA|        70|           0|         0|        1000|      1000|    53033032601|\n",
      "|530559603003033|   WA|        70|          25|        25|        1000|      1000|    53055960300|\n",
      "|530090009001000|   WA|        70|           0|         0|        1000|      1000|    53009000900|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removes last 4 characters of census_tract_column:\n",
    "\n",
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "\n",
    "# # def remove_if_fifteen(input_df):\n",
    "\n",
    "# # df_BROADBAND_WA_ONLY = df_BROADBAND_WA_ONLY.withColumn(\"census_tract\",expr(\"substring(census_tract, 1, length(census_tract)-4)\"))when(len(df_BROADBAND_WA_ONLY[\"session\"]) == 15)\n",
    "\n",
    "# df = df_BROADBAND_WA_ONLY.withColumn(\"census_tract\",expr(\"substring(census_tract, 1, length(census_tract)-4)\"))\n",
    "\n",
    "df_eleven_tract_WA = df_BROADBAND_WA_ONLY.withColumn(\"census_tract_11\",expr(\"substring(census_tract, 1, 11)\"))\n",
    "\n",
    "# # df_BROADBAND_WA_ONLY.show()\n",
    "\n",
    "# df = output_df_broadband.withColumn('census_tract', output_df_broadband.substring(expr.col('census_tract'), 1, 11))\n",
    "# df = output_df_broadband.withColumn('census_tract', df['census_tract'].substr(1, 11))\n",
    "\n",
    "# from pyspark.sql.functions import substring, length\n",
    "\n",
    "# df = df_na_dropped.withColumn(\"census_tract\", when(length(df_na_dropped.census_tract) > 11, substring(df_na_dropped.census_tract, 1, length(df_na_dropped.census_tract) - 4)).otherwise(df_na_dropped.census_tract))\n",
    "\n",
    "# df = df_na_dropped.substring(df_na_dropped.col('census_tract'), 1, 11) in place of df_na_dropped['census_tract'].substr(1, 11)\n",
    "df_eleven_tract_WA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+-----------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|census_tract_11|tech_binary|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+-----------+\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|    53077001800|          0|\n",
      "|530330304013022|   WA|        70|           0|         0|        1000|      1000|    53033030401|          0|\n",
      "|530439604003069|   WA|        70|          25|        25|        1000|      1000|    53043960400|          0|\n",
      "|530610402002017|   WA|        70|          25|        25|        1000|      1000|    53061040200|          0|\n",
      "|530730102002089|   WA|        70|          25|        25|        1000|      1000|    53073010200|          0|\n",
      "|530750009003056|   WA|        70|          25|        25|        1000|      1000|    53075000900|          0|\n",
      "|530330029003005|   WA|        70|           0|         0|        1000|      1000|    53033002900|          0|\n",
      "|530330320104003|   WA|        70|           0|         0|        1000|      1000|    53033032010|          0|\n",
      "|530250109011002|   WA|        70|          25|        25|        1000|      1000|    53025010901|          0|\n",
      "|530730101001095|   WA|        70|          25|        25|        1000|      1000|    53073010100|          0|\n",
      "|530479704003101|   WA|        70|           0|         0|        1000|      1000|    53047970400|          0|\n",
      "|530250114024024|   WA|        70|          25|        25|        1000|      1000|    53025011402|          0|\n",
      "|530179502001205|   WA|        70|          25|        25|        1000|      1000|    53017950200|          0|\n",
      "|530330323201017|   WA|        70|           0|         0|        1000|      1000|    53033032320|          0|\n",
      "|530530729052021|   WA|        70|           0|         0|        1000|      1000|    53053072905|          0|\n",
      "|530150016002073|   WA|        12|          50|         5|          50|         5|    53015001600|          0|\n",
      "|530479703001218|   WA|        70|           0|         0|        1000|      1000|    53047970300|          0|\n",
      "|530330326011048|   WA|        70|           0|         0|        1000|      1000|    53033032601|          0|\n",
      "|530559603003033|   WA|        70|          25|        25|        1000|      1000|    53055960300|          0|\n",
      "|530090009001000|   WA|        70|           0|         0|        1000|      1000|    53009000900|          0|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext.registerDataFrameAsTable(df_eleven_tract_WA, \"broadband_table\")\n",
    "\n",
    "# df_broadband_WA = sqlContext.sql(\"SELECT census_tract AS census_block, technology, mc_downspeed, ma_downspeed, census_tract_11 as census_tract from broadband_table\")\n",
    "# df_broadband_WA.show(5)\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_eleven_tract_WA\\\n",
    ".select(\"*\", when(df_broadband_WA.technology == 50, 1)\\\n",
    "        .otherwise(0)\\\n",
    "        .alias(\"tech_binary\"))\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "u\"\\nmismatched input 'CASE' expecting <EOF>(line 1, pos 93)\\n\\n== SQL ==\\nSELECT census_block, technology, mc_downspeed, ma_downspeed, census_tract_11 as census_tract CASE WHEN technology = 50 THEN 'FIBER' ELSE Quantity = 'NOT FIBER' FROM broadband_table\\n---------------------------------------------------------------------------------------------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-8e9d3722a7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df_broadband_WA = sqlContext.sql(\n\u001b[0;32m----> 2\u001b[0;31m                 \"SELECT census_block, technology, mc_downspeed, ma_downspeed, census_tract_11 as census_tract CASE WHEN technology = 50 THEN 'FIBER' ELSE Quantity = 'NOT FIBER' FROM broadband_table\")\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_broadband_WA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: u\"\\nmismatched input 'CASE' expecting <EOF>(line 1, pos 93)\\n\\n== SQL ==\\nSELECT census_block, technology, mc_downspeed, ma_downspeed, census_tract_11 as census_tract CASE WHEN technology = 50 THEN 'FIBER' ELSE Quantity = 'NOT FIBER' FROM broadband_table\\n---------------------------------------------------------------------------------------------^^^\\n\""
     ]
    }
   ],
   "source": [
    "df_broadband_WA = sqlContext.sql(\n",
    "                \"SELECT census_block, technology, mc_downspeed, ma_downspeed, census_tract_11 as census_tract CASE WHEN technology = 50 THEN 'FIBER' ELSE Quantity = 'NOT FIBER' FROM broadband_table\")\n",
    "df_broadband_WA.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|census_tract|\n",
      "+------------+\n",
      "| 53073000502|\n",
      "| 53053061001|\n",
      "| 53063003200|\n",
      "| 53077003002|\n",
      "| 53033032007|\n",
      "| 53011041331|\n",
      "| 53015001300|\n",
      "| 53067012211|\n",
      "| 53077002200|\n",
      "| 53021020100|\n",
      "| 53063001100|\n",
      "| 53065951300|\n",
      "| 53061052607|\n",
      "| 53011040703|\n",
      "| 53035091204|\n",
      "| 53033003000|\n",
      "| 53025010100|\n",
      "| 53011040710|\n",
      "| 53047970600|\n",
      "| 53061053700|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_broadband_WA.select(\"census_tract\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378559"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_broadband_WA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "database_name = 'broadband_scoutdb'\n",
    "hostname = 'ec2-54-186-79-112.us-west-2.compute.amazonaws.com'\n",
    "url = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)\n",
    "properties = {\"user\": \"postgres\",\"password\": \"postgres\",\"driver\": \"org.postgresql.Driver\"}\n",
    "df_eleven_tract_WA.write.jdbc(url=url, table=\"broadband_table_WA\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT the distinct values in Washington Only for ma_downspeed with null values removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "|census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "|      195574|    1|        12|          35|        25|          49|        44|\n",
      "+------------+-----+----------+------------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "count_distinct_vals = df_BROADBAND_WA_ONLY.agg(*(countDistinct(col(ma_upspeed)).alias(ma_upspeed) for ma_upspeed in df_BROADBAND_WA_ONLY.columns))\n",
    "\n",
    "count_distinct_vals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'countDistinct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f349e77ec8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#DOWNSPEED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_distinct_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_downspeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_downspeed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mma_upspeed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_distinct_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-f349e77ec8db>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((ma_upspeed,))\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#DOWNSPEED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount_distinct_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountDistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_downspeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_downspeed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mma_upspeed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_BROADBAND_WA_ONLY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_distinct_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'countDistinct' is not defined"
     ]
    }
   ],
   "source": [
    "#DOWNSPEED\n",
    "count_distinct_vals = df_BROADBAND_WA_ONLY.agg(*(countDistinct(col(mc_downspeed)).alias(mc_downspeed) for ma_upspeed in df_BROADBAND_WA_ONLY.columns))\n",
    "\n",
    "count_distinct_vals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length of census tract column to check if all values are 15 or if some are 11 already, creates new column tract_length\n",
    "# all lengths of the census tract column\n",
    "\n",
    "# df_count_tract_col_lengths = output_df_broadband.select('*',size(output_df_broadband['census_tract']).alias('tract_length'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BROADBAND_WA_ONLY['technology'] = df_BROADBAND_WA_ONLY['technology'].replace(\"50\", \"1\")\n",
    "df_BROADBAND_WA_ONLY['technology'] = df_BROADBAND_WA_ONLY['technology'].replace(!=\"50\", \"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|   census_tract|state|technology|ma_downspeed|ma_upspeed|mc_downspeed|mc_upspeed|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "|530770018004053|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330304013022|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530439604003069|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530610402002017|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730102002089|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530750009003056|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530330029003005|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530330320104003|   WA|        70|           0|         0|        1000|      1000|\n",
      "|530250109011002|   WA|        70|          25|        25|        1000|      1000|\n",
      "|530730101001095|   WA|        70|          25|        25|        1000|      1000|\n",
      "+---------------+-----+----------+------------+----------+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_BROADBAND_WA_ONLY.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group broadband table by shortened 11 character census tract, and get median value of mc_downspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+\n",
      "|census_tract|technology|med_mc_downspeed|\n",
      "+------------+----------+----------------+\n",
      "| 53005010201|        70|             100|\n",
      "| 53033021700|        70|            1000|\n",
      "| 53033025302|        70|            1000|\n",
      "| 53053062400|        70|            1000|\n",
      "| 53053063400|        42|               0|\n",
      "| 53053072112|        42|               0|\n",
      "| 53063000500|        70|               5|\n",
      "| 53063010202|        70|              30|\n",
      "| 53073000400|        70|              25|\n",
      "| 53073010301|        70|            1000|\n",
      "+------------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df_eleven_tract_WA.registerTempTable(\"broadband_table_wa\")\n",
    "df_BROADBAND_WA_grouped = sqlContext.sql(\"select census_tract, technology, percentile_approx(mc_downspeed, 0.5) as med_mc_downspeed from broadband_table_wa group by census_tract, technology\")\n",
    "\n",
    "df_BROADBAND_WA_grouped.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11228"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BROADBAND_WA_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df_BROADBAND_WA_grouped.registerTempTable(\"broadband_table_wa\")\n",
    "df_BROADBAND_WA_grouped = sqlContext.sql(\"select census_tract, percentile_approx(mc_downspeed, 0.5) as med_mc_downspeed from broadband_table_wa group by census_tract\")\n",
    "\n",
    "df_BROADBAND_WA_grouped.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "pathname_input_TRACT_ZIP = 's3a://sparkforinsightproject/TRACT_ZIP_122017.csv'\n",
    "\n",
    "\n",
    "def etl_tract_to_zip(input_TRACT_ZIP_csv):\n",
    "    \n",
    "    census_zip_schema = StructType([\n",
    "    StructField('tract', StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"bus_ratio\", FloatType(), True)\n",
    "        ])\n",
    "\n",
    "    df_TRACT_ZIP = spark.read.csv(input_TRACT_ZIP_csv, header=True, schema=census_zip_schema, sep=',', nullValue='NA')\\\n",
    "                                .withColumnRenamed(\"tract\", \"census_tract\")\n",
    "\n",
    "    return df_TRACT_ZIP\n",
    "\n",
    "\n",
    "df_TRACT_ZIP_output = etl_tract_to_zip(pathname_input_TRACT_ZIP)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+------------+-----+-----------+\n",
      "|census_tract|technology|med_mc_downspeed|census_tract|  zip|  bus_ratio|\n",
      "+------------+----------+----------------+------------+-----+-----------+\n",
      "| 53011041331|        11|               0| 53011041331|98682|        1.0|\n",
      "| 53011041331|        60|               0| 53011041331|98682|        1.0|\n",
      "| 53011041331|        30|              10| 53011041331|98682|        1.0|\n",
      "| 53011041331|        70|            1000| 53011041331|98682|        1.0|\n",
      "| 53011041331|        43|               0| 53011041331|98682|        1.0|\n",
      "| 53011041331|        50|               0| 53011041331|98682|        1.0|\n",
      "| 53011041331|        12|               0| 53011041331|98682|        1.0|\n",
      "| 53015001300|        43|               0| 53015001300|98626|        1.0|\n",
      "| 53015001300|        50|            1000| 53015001300|98626|        1.0|\n",
      "| 53015001300|        60|               0| 53015001300|98626|        1.0|\n",
      "| 53015001300|        11|               0| 53015001300|98626|        1.0|\n",
      "| 53015001300|        10|               0| 53015001300|98626|        1.0|\n",
      "| 53033032007|        11|               0| 53033032007|98042| 0.22625215|\n",
      "| 53033032007|        11|               0| 53033032007|98058|0.028929189|\n",
      "| 53033032007|        11|               0| 53033032007|98038|  0.7448186|\n",
      "| 53033032007|        10|               0| 53033032007|98042| 0.22625215|\n",
      "| 53033032007|        10|               0| 53033032007|98058|0.028929189|\n",
      "| 53033032007|        10|               0| 53033032007|98038|  0.7448186|\n",
      "| 53033032007|        60|               0| 53033032007|98042| 0.22625215|\n",
      "| 53033032007|        60|               0| 53033032007|98058|0.028929189|\n",
      "+------------+----------+----------------+------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join broaband_table_WA with ct to zip code data\n",
    "\n",
    "# left_join_BROADBAND_ZIP_ADDED = df_eleven_tract_WA.join(df_TRACT_ZIP_output, df_eleven_tract_WA.census_tract == df_TRACT_ZIP_output.census_tract,how='left') # Could also use 'left_outer'\n",
    "# left_join_BROADBAND_ZIP_ADDED.show()\n",
    "\n",
    "\n",
    "# left_join_BROADBAND_ZIP_ADDED = df_BROADBAND_WA_grouped.join(df_TRACT_ZIP_output,[df_BROADBAND_WA_grouped.census_tract==df_TRACT_ZIP_output.census_tract, df_BROADBAND_WA_grouped.census_tract==df_TRACT_ZIP_output.census_tract],'left')\n",
    "\n",
    "left_join_BROADBAND_ZIP_ADDED = df_broadband_WA.join(df_TRACT_ZIP_output, df_BROADBAND_WA_grouped.census_tract == df_TRACT_ZIP_output.census_tract)\n",
    "\n",
    "left_join_BROADBAND_ZIP_ADDED.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+------------+-----+---------+\n",
      "|census_tract|technology|med_mc_downspeed|census_tract|  zip|bus_ratio|\n",
      "+------------+----------+----------------+------------+-----+---------+\n",
      "| 53011041331|        11|               0| 53011041331|98682|      1.0|\n",
      "| 53011041331|        60|               0| 53011041331|98682|      1.0|\n",
      "| 53011041331|        30|              10| 53011041331|98682|      1.0|\n",
      "| 53011041331|        70|            1000| 53011041331|98682|      1.0|\n",
      "| 53011041331|        43|               0| 53011041331|98682|      1.0|\n",
      "+------------+----------+----------------+------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "http://southpark.cc.com/full-episodes/s22e09-unfulfilled#source=2b6c5ab4-d717-4e84-9143-918793a3b636:63a32034-1ea6-492d-b95b-9433e3f62f8d&position=2&sort=airdate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----+------+\n",
      "|census_tract|state|year|   hpi|\n",
      "+------------+-----+----+------+\n",
      "| 53001950100|   WA|2017|121.51|\n",
      "| 53001950300|   WA|2017|250.55|\n",
      "| 53001950400|   WA|2017|204.59|\n",
      "| 53001950500|   WA|2017|218.37|\n",
      "| 53003960100|   WA|2017|258.93|\n",
      "+------------+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    # If PYTHONPATH is not set, findspark and findspark.init() will find it on your machine \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import spark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import sys\n",
    "# schemaString = 'something'\n",
    "\n",
    "# fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local') \\\n",
    "        .appName(\"BroadbandScout\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "pathname_input = 's3a://sparkforinsightproject/HPI_AT_BDL_tract.csv'\n",
    "\n",
    "pathname_output = 's3a://sparkforinsightproject/database_data/sparkdf_broadband_output_2'\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def etl_hpi_to_tract(input_data_txt):\n",
    "    \n",
    "\n",
    "    rdd_HPI_to_CENSUS = sc.textFile(input_data_txt)\n",
    "\n",
    "    # This file is a comma separated file and has some commas embedded within string objects within some of the columns. The file is first read in to       # an RDD and this function is used in a map transformation (below) to           # remove those commas.\n",
    "\n",
    "#     def remove_commas(row):\n",
    "\n",
    "#         row = re.sub('(?!(([^\"]*\"){2})*[^\"]*$),', '', row)\n",
    "\n",
    "#         return row\n",
    "\n",
    "#     # The first 11 numbers of a census block number match the 11 numbers in a census tract number. While the census block numbers in this file are 15       # digits long, the other datasets to be joined with this file for the           # database (containing housing data and zip codes) only have the 11 digit       # census tract data. Therefore, this function is used in a map                  # transformation (below) to keep only the first 11 numbers of the census        # block numbers. This column will be used to join this file by other files      # containing census tract numbers.\n",
    "\n",
    "\n",
    "#     def reduce_census_block_code(row):\n",
    "\n",
    "#         row[9] = row[9][:11]\n",
    "\n",
    "#         return row\n",
    "\n",
    "#     # This block removes the header, which is necessary for implementing the code below.\n",
    "\n",
    "# #    header = rdd_BROADBAND.first()\n",
    "\n",
    "# #    rdd_BROADBAND = rdd_BROADBAND.filter(lambda line: line != header)\n",
    "\n",
    "\n",
    "#     # This block of code removes ascii unicode information, removes commas embedded in within strings in some of the columns, splits the RDD object      # on the commas, and reduces the length of the census blocks from 15 to 11      # digits.\n",
    "\n",
    "    rdd_HPI_to_TRACT = rdd_HPI_to_CENSUS\\\n",
    "                                   .map(lambda x: x.encode('ascii', 'ignore'))\\\n",
    "                                   .map(lambda x: x.split(','))\\\n",
    "                                   .map(lambda l: (l[0], l[1], l[2], l[4]))\n",
    "\n",
    "    \n",
    "\n",
    "    hpi_schema = StructType([\n",
    "        StructField(\"tract\", StringType(), True),\n",
    "        StructField(\"state_abbr\", StringType(), True),\n",
    "        StructField(\"year\", StringType(), True),\n",
    "        StructField(\"annual_change\", StringType(), True),\n",
    "        StructField(\"hpi\", FloatType(), True),\n",
    "        StructField(\"hpi1990\", FloatType(), True),\n",
    "        StructField(\"hpi2000\", FloatType(), True)\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_HPI_to_TRACT = spark.read.csv(input_data_txt, header=True, schema=hpi_schema)#\\\n",
    "#                         .withColumn(\"('tract'\",regexp_replace(col(\"('tract'\"), \"\\(\", \"\"))\\\n",
    "#                         .withColumn(\"'hpi')\",regexp_replace(col(\"'hpi')\"), \"\\)\", \"\")) \n",
    "                                        \n",
    "    \n",
    "    #, quote='\"', header=True, mode=\"DROPMALFORMED\", sep=',', nullValue='NA', schema=broadband_schema\n",
    "\n",
    "#     df_transformed_BROADBAND = spark.read.csv(rdd_transformed_BROADBAND, header=False,\n",
    "#                                               mode=\"DROPMALFORMED\", sep=',')\n",
    "\n",
    "    # This code renames the columns I want to keep.\n",
    "\n",
    "    df_HPI_to_TRACT = df_HPI_to_TRACT\\\n",
    "                               .withColumnRenamed(\"tract\", \"census_tract\")\\\n",
    "                               .withColumnRenamed(\"state_abbr\", \"state\")\n",
    "\n",
    "\n",
    "    # This code selects and saves just seven of the 16 columns from the \n",
    "    # original file that have some clear potential value for the database.\n",
    "\n",
    "    df_HPI_to_TRACT = df_HPI_to_TRACT\\\n",
    "                                       .select(\n",
    "                                       \"census_tract\",\n",
    "                                       \"state\",\n",
    "                                       \"year\",\n",
    "                                       \"hpi\")\n",
    "#                                        \"ma_downspeed\",\\\n",
    "#                                        \"ma_upspeed\",\\\n",
    "#                                        \"mc_downspeed\",\\\n",
    "#                                        \"mc_upspeed\")\n",
    "\n",
    "    return df_HPI_to_TRACT\n",
    "\n",
    "# def main():\n",
    "#     input_data_txt = sys.argv[1]\n",
    "#     output_data_txt = sys.argv[2]\n",
    "#     extract_transform_load_broadband(input_data_txt, output_data_txt)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_hpi_to_tract = etl_hpi_to_tract(pathname_input)\n",
    "\n",
    "\n",
    "df_hpi_tract_2017_wa = output_hpi_to_tract.where(col('year').isin({'2017'})).where(col('state').isin({'WA'}))\n",
    "\n",
    "df_hpi_tract_2017_wa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Reference 'census_tract' is ambiguous, could be: broadband_table_wa.census_tract, census_tract.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d618d38fd357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minner_join_broadband_hpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_join_BROADBAND_ZIP_ADDED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_hpi_tract_2017_wa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_join_BROADBAND_ZIP_ADDED\u001b[0m\u001b[0;34m.\u001b[0m            \u001b[0mcensus_tract\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdf_hpi_tract_2017_wa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcensus_tract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_join_BROADBAND_ZIP_ADDED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcensus_tract\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdf_hpi_tract_2017_wa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcensus_tract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[1;32m   1182\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m-> 1183\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Reference 'census_tract' is ambiguous, could be: broadband_table_wa.census_tract, census_tract.;\""
     ]
    }
   ],
   "source": [
    "\n",
    "inner_join_broadband_hpi = left_join_BROADBAND_ZIP_ADDED.join(df_hpi_tract_2017_wa,[left_join_BROADBAND_ZIP_ADDED.\\\n",
    "            census_tract==df_hpi_tract_2017_wa.census_tract, left_join_BROADBAND_ZIP_ADDED.census_tract==df_hpi_tract_2017_wa.census_tract],'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_join_BROADBAND_ZIP_ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "database_name = 'broadband_scoutdb'\n",
    "hostname = 'ec2-54-186-79-112.us-west-2.compute.amazonaws.com'\n",
    "url = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)\n",
    "properties = {\"user\": \"postgres\",\"password\": \"postgres\",\"driver\": \"org.postgresql.Driver\"}\n",
    "left_join_BROADBAND_ZIP_ADDED.write.jdbc(url=url, table=\"broadband_table_zip\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mode = \"overwrite\"\n",
    "# database_name = 'broadband_scoutdb'\n",
    "# hostname = 'ec2-54-186-79-112.us-west-2.compute.amazonaws.com'\n",
    "# url = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)\n",
    "# properties = {\"user\": \"postgres\",\"password\": \"postgres\",\"driver\": \"org.postgresql.Driver\"}\n",
    "# broadband_output.write.jdbc(url=url, table=\"broadband_table\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o78.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:499)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3406da621ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"postgres\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"org.postgresql.Driver\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbroadband_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:499)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://198.123.43.24:5432/kockpit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o302.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:499)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f33529fe2cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbroadband_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"broadband_table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o302.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:499)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "broadband_output.write.jdbc(url=url, table=\"broadband_table\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(broadband_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-f8b3444c-69c1-494c-9b27-6f89a0a0e471.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import (SparkSession, \n",
    "                         functions as F,\n",
    "                        )\n",
    "import postgres\n",
    "import argparse\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "pathname_input = 's3a://sparkforinsightproject/Fixed_Broadband_Deployment_Data__June__2017_Status_V1.csv'\n",
    "\n",
    "\n",
    "\n",
    "class transform_BROADBAND_data(object):\n",
    "    def __init__(self, file_name):\n",
    "        self.spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .master('local') \\\n",
    "            .appName(\"BroadbandScout\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        broadband_schema = StructType([\n",
    "            StructField(\"Logical Record Number\", StringType(), True),\n",
    "            StructField(\"Provider ID\", StringType(), True),\n",
    "            StructField(\"FRN\", StringType(), True),\n",
    "            StructField(\"Provider Name\", StringType(), True),\n",
    "            StructField(\"DBA Name\", StringType(), True),\n",
    "            StructField(\"Holding Company Name\", StringType(), True),\n",
    "            StructField(\"Holding Company Number\", StringType(), True),\n",
    "            StructField(\"Holding Company Final\", StringType(), True),\n",
    "            StructField(\"State\", StringType(), True),\n",
    "            StructField(\"Census Block FIPS Code\", StringType(), True),\n",
    "            StructField(\"Technology Code\", StringType(), True),\n",
    "            StructField(\"Consumer\", StringType(), True),\n",
    "            StructField(\"Max Advertised Downstream Speed (mbps)\", IntegerType(), True),\n",
    "            StructField(\"Max Advertised Upstream Speed (mbps)\", IntegerType(), True),\n",
    "            StructField(\"Business\", StringType(), True),\n",
    "            StructField(\"Max CIR Downstream Speed (mbps)\", IntegerType(), True),\n",
    "            StructField(\"Max CIR Upstream Speed (mbps)\", IntegerType(), True)\n",
    "            ])\n",
    "\n",
    "\n",
    "    input_file_directory = 's3a://sparkforinsightproject/'\n",
    "    input_filename = 'Fixed_Broadband_Deployment_Data__June__2017_Status_V1.csv'\n",
    "   \n",
    "    def read_BROADBAND_csv(self):\n",
    "        file_name = \"{0}/{file_name}\".format(file_directory, input_filename)\n",
    "        return self.spark.read.csv(input_data_txt, quote='\"', header=True,\n",
    "                                     mode=\"DROPMALFORMED\", sep=',', nullValue='NA', schema=broadband_schema)\n",
    "\n",
    "\n",
    "    def df_transformer(self):\n",
    "        df_transformed_BROADBAND = df_transformed_BROADBAND\\\n",
    "           .withColumnRenamed(\"State\", \"state\")\\\n",
    "           .withColumnRenamed(\"Census Block FIPS Code\", \"census_tract\")\\\n",
    "           .withColumnRenamed(\"Technology Code\", \"technology\")\\\n",
    "           .withColumnRenamed(\"Max Advertised Downstream Speed (mbps)\", \"ma_downspeed\")\\\n",
    "           .withColumnRenamed(\"Max Advertised Upstream Speed (mbps)\", \"ma_upspeed\")\\\n",
    "           .withColumnRenamed(\"Max CIR Downstream Speed (mbps)\",  \"mc_downspeed\")\\\n",
    "           .withColumnRenamed(\"Max CIR Upstream Speed (mbps)\",  \"mc_upspeed\")\n",
    "\n",
    "\n",
    "        # This code selects and saves just seven of the 16 columns from the original \n",
    "        # file that have some clear potential value for the database.\n",
    "\n",
    "        df_transformed_BROADBAND = df_transformed_BROADBAND\\\n",
    "           .select(\n",
    "           \"census_tract\",\\\n",
    "           \"state\",\\\n",
    "           \"technology\",\\\n",
    "           \"ma_downspeed\",\\\n",
    "           \"ma_upspeed\",\\\n",
    "           \"mc_downspeed\",\\\n",
    "           \"mc_upspeed\")\n",
    "        \n",
    "        return df_transformed_BROADBAND\n",
    "    \n",
    "    def write_to_postgresql(self, out_df):\n",
    "        table = \"broadband_table\"\n",
    "        mode = \"append\"\n",
    "        \n",
    "        connector = postgres.PostgresConnector()\n",
    "        connector.write(out_df, table, mode)\n",
    "\n",
    "    def run(self):\n",
    "        csv_df = self.read_BROADBAND_csv()\n",
    "        csv_df.printSchema()\n",
    "        \n",
    "        out_df = self.df_transformer(csv_df)\n",
    "        out_df.printSchema()\n",
    "        \n",
    "        self.write_to_postgresql(out_df)\n",
    "        \n",
    "\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser()\n",
    "#         parser.add_argument(\"--debug\", help=\"debug mode, loads small test file.\", action=\"store_true\")\n",
    "#         parser.add_argument(\"--file\", help=\"file name to process\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    file_name = args.file if args.file else input_filename\n",
    "    proc = transform_BROADBAND_data(file_name)\n",
    "    proc.run()\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a9349a4ff092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a9349a4ff092>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#         parser.add_argument(\"--debug\", help=\"debug mode, loads small test file.\", action=\"store_true\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#         parser.add_argument(\"--file\", help=\"file name to process\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \"\"\"\n\u001b[1;32m   2373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: error: %s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates \"BroadbandScout\" as my broadband_table for the BroadbandScout project.\n",
    "\n",
    "broadband_output.createOrReplaceTempView(\"broadband_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n"
     ]
    }
   ],
   "source": [
    "print('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pandas select all columns from the broadband dataframe and creates a pyspark.sql.dataframe.Dataframe\n",
    "\n",
    "broadband_full_table = spark.sql('SELECT * \\\n",
    "FROM broadband_table')\n",
    "\n",
    "# broadband_full_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'pyspark.sql.dataframe.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-828f2e01eaa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd_df_broadband\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadband_full_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 413\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Invalid file path or buffer object type: {_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df_broadband = pd.read_csv(broadband_full_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o120.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 47 in stage 1.0 failed 1 times, most recent failure: Lost task 47.0 in stage 1.0 (TID 48, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:247)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1346)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:247)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1346)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54ad0aef066f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Converts pyspark.sql.dataframe to a Pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd_df_broadband\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadband_full_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o120.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 47 in stage 1.0 failed 1 times, most recent failure: Lost task 47.0 in stage 1.0 (TID 48, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:247)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1346)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:247)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1346)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Converts pyspark.sql.dataframe to a Pandas dataframe\n",
    "\n",
    "pd_df_broadband = broadband_full_table.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df_broadband.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a424fb66c593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postgresql://{0}:{1}@{2}:5432/{3}'\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postgres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'postgres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ec2-54-186-79-112.us-west-2.compute.amazonaws.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'broadband_scoutDB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'broadband_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from sqlalchemy import create_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_sql'"
     ]
    }
   ],
   "source": [
    "# This code only works with a pandas dataframe, not a spark dataframe\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://{0}:{1}@{2}:5432/{3}'\\\n",
    "        .format('postgres', 'postgres', 'ec2-54-186-79-112.us-west-2.compute.amazonaws.com', 'broadband_scoutDB'))\n",
    "nto.to_sql('broadband_table', engine)\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('postgresql://username:password@localhost:5432/name_of_db')\n",
    "# df.to_sql('table_name', engine)\n",
    "\n",
    "# import pymysql\n",
    "# from sqlalchemy import create_engine\n",
    "# cnx = create_engine('mysql+pymysql://[user]:[pass]@[host]:[port]/[schema]', echo=False)\n",
    "\n",
    "# data = pd.read_sql('SELECT * FROM sample_table', cnx)\n",
    "# data.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n",
    "\n",
    "\n",
    "# import mysql.connector\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# engine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\n",
    "# data.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postgres\n",
    "\n",
    "pathname_input = 's3a://sparkforinsightproject/Fixed_Broadband_Deployment_Data__June__2017_Status_V1.csv'\n",
    "\n",
    "table = \"broadband_table\"\n",
    "mode = \"append\"\n",
    "\n",
    "\n",
    "\n",
    "input_directory = 's3a://sparkforinsightproject/'\n",
    "\n",
    "input_filename = 'Fixed_Broadband_Deployment_Data__June__2017_Status_V1.csv'\n",
    "\n",
    "\n",
    "connector = postgres.PostgresConnector()\n",
    "connector.write(out_df, table, mode)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args()\n",
    "\n",
    "file_name = args.file if args.file else input_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "cnx = create_engine('mysql+pymysql://[user]:[pass]@[host]:[port]/[schema]', echo=False)\n",
    "\n",
    "data = pd.read_sql('SELECT * FROM sample_table', cnx)\n",
    "data.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(census_tract=u'060770053034016', state=u'CA', technology_code=u'70', max_adver_downstr_speed=u'21', max_adver_upstr_speed=u'5', max_cir_downstr_speed=u'200', max_cir_upstr_speed=u'200'),\n",
       " Row(census_tract=u'550590029041059', state=u'WI', technology_code=u'70', max_adver_downstr_speed=u'5', max_adver_upstr_speed=u'1', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'490351031002010', state=u'UT', technology_code=u'30', max_adver_downstr_speed=u'12', max_adver_upstr_speed=u'12', max_cir_downstr_speed=u'12', max_cir_upstr_speed=u'12'),\n",
       " Row(census_tract=u'340155016082005', state=u'NJ', technology_code=u'50', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'100', max_cir_upstr_speed=u'100'),\n",
       " Row(census_tract=u'371139706003009', state=u'NC', technology_code=u'42', max_adver_downstr_speed=u'100', max_adver_upstr_speed=u'5', max_cir_downstr_speed=u'100', max_cir_upstr_speed=u'5'),\n",
       " Row(census_tract=u'360210004011070', state=u'NY', technology_code=u'11', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'1.5', max_cir_downstr_speed=u'25', max_cir_upstr_speed=u'1.5'),\n",
       " Row(census_tract=u'320310012012053', state=u'NV', technology_code=u'70', max_adver_downstr_speed=u'12', max_adver_upstr_speed=u'3', max_cir_downstr_speed=u'0', max_cir_upstr_speed=u'0'),\n",
       " Row(census_tract=u'170759506001009', state=u'IL', technology_code=u'70', max_adver_downstr_speed=u'5', max_adver_upstr_speed=u'1', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'490451307011026', state=u'UT', technology_code=u'70', max_adver_downstr_speed=u'15', max_adver_upstr_speed=u'3', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530770018004053', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_full_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "u\"\\nmismatched input 'UPDATE' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}(line 1, pos 0)\\n\\n== SQL ==\\nUPDATE BroadbandScout     SET max_adver_upstream_speed=LEFT(max_adver_upstream_speed, LEN(max_adver_upstream_speed)-4)\\n^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-172930e24fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m broadband_full_table_ct11 = spark.sql(\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m'\u001b[0m\u001b[0mUPDATE\u001b[0m \u001b[0mBroadbandScout\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     SET max_adver_upstream_speed=LEFT(max_adver_upstream_speed, LEN(max_adver_upstream_speed)-4)')\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: u\"\\nmismatched input 'UPDATE' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}(line 1, pos 0)\\n\\n== SQL ==\\nUPDATE BroadbandScout     SET max_adver_upstream_speed=LEFT(max_adver_upstream_speed, LEN(max_adver_upstream_speed)-4)\\n^^^\\n\""
     ]
    }
   ],
   "source": [
    "broadband_full_table_ct11 = spark.sql(\n",
    "    'UPDATE BroadbandScout \\\n",
    "    SET max_adver_upstream_speed=LEFT(max_adver_upstream_speed, LEN(max_adver_upstream_speed)-4)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'broadband_full_table_ct11' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e2a4e757e654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbroadband_full_table_ct11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'broadband_full_table_ct11' is not defined"
     ]
    }
   ],
   "source": [
    "broadband_full_table_ct11.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas select all columns ONLY WHERE state = WA from the broadband dataframe\n",
    "\n",
    "broadband_full_table = spark.sql('SELECT * \\\n",
    "FROM BroadbandScout \\\n",
    "WHERE state = \"WA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(census_tract=u'530770018004053', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330304013022', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530439604003069', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530610402002017', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530730102002089', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530750009003056', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330029003005', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330320104003', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530250109011002', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530730101001095', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530479704003101', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530250114024024', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530179502001205', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330323201017', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530530729052021', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530150016002073', state=u'WA', technology_code=u'12', max_adver_downstr_speed=u'50', max_adver_upstr_speed=u'5', max_cir_downstr_speed=u'50', max_cir_upstr_speed=u'5'),\n",
       " Row(census_tract=u'530479703001218', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330326011048', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530559603003033', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530090009001000', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530630144001009', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530530730053010', state=u'WA', technology_code=u'12', max_adver_downstr_speed=u'50', max_adver_upstr_speed=u'10', max_cir_downstr_speed=u'50', max_cir_upstr_speed=u'10'),\n",
       " Row(census_tract=u'530379754021035', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530479704002057', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530530713064003', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530179501001029', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'25', max_adver_upstr_speed=u'25', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330235003005', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330239003038', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530099400002017', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000'),\n",
       " Row(census_tract=u'530330019001016', state=u'WA', technology_code=u'70', max_adver_downstr_speed=u'0', max_adver_upstr_speed=u'0', max_cir_downstr_speed=u'1000', max_cir_upstr_speed=u'1000')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_full_table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average value of max_adver_downstr_speed\n",
    "\n",
    "                                                                                             \n",
    "broadband_results = spark.sql('SELECT avg(max_adver_downstr_speed) \\\n",
    "FROM BroadbandScout \\\n",
    "WHERE state = \"WA\"')\n",
    "\n",
    "pd_df_broadband = broadband_results.toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max_adver_downstr_speed and housing values joined by census tract number\n",
    "\n",
    "broadband_output.createOrReplaceTempView(\"BroadbandScout\")\n",
    "                                                                                             \n",
    "broadband_results = spark.sql('SELECT avg(max_adver_downstr_speed) \\\n",
    "FROM BroadbandScout \\\n",
    "WHERE state = \"WA\"')\n",
    "\n",
    "pd_df_broadband = broadband_results.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_df_broadband' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6a63b11ae73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pd_df_broadband = pd_df_broadband.sort_values('max_adver_downstr_speed')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd_df_broadband\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_df_broadband' is not defined"
     ]
    }
   ],
   "source": [
    "# pd_df_broadband = pd_df_broadband.sort_values('max_adver_downstr_speed')\n",
    "\n",
    "pd_df_broadband.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "class PostgresConnector(object):\n",
    "    def __init__(self):\n",
    "        self.database_name = 'broadband_scoutdb'\n",
    "        self.hostname = 'ec2-54-186-79-112.us-west-2.compute.amazonaws.com'\n",
    "        self.url_connect = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=self.hostname, db=self.database_name)\n",
    "        self.properties = {\"user\":\"thomas_ernste\",\n",
    "                      \"driver\": \"org.postgresql.Driver\"\n",
    "                     }\n",
    "    def get_writer(self, df):\n",
    "        return DataFrameWriter(df)\n",
    "        \n",
    "    def write(self, df, table, mode):\n",
    "        my_writer = self.get_writer(df)\n",
    "        my_writer.jdbc(self.url_connect, table, mode, self.properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# \"password\":os.environ['POSTGRES_PASS'],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(host = \"ec2-54-186-79-112.us-west-2.compute.amazonaws.com\",\\\n",
    "database=\"broadband_scoutdb\",/\n",
    "user='thomas_ernste', \n",
    "password=')\n",
    "\n",
    "#cursor\n",
    "cur = con.cursor()\n",
    "\n",
    "#example 1 execute a query\n",
    "cur.execute(\"insert into employees \")\n",
    "\n",
    "#example 1 execute a query\n",
    "$execute query\n",
    "cur.execute('SELECT * FROM broadband_table')\n",
    "\n",
    "\n",
    "rows = cur.fetchall()\n",
    "\n",
    "for r in rows:\n",
    "    print(f\"id {r[0]} name {r[1]}\")\n",
    "\n",
    "#close the cursor\n",
    "cur = con.cursor()\n",
    "\n",
    "#close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from pyspark.sql import DataFrameWriter\n",
    "import os\n",
    "\n",
    "class PostgresConnector(object):\n",
    "    def __init__(self):\n",
    "        self.database_name = 'occupancy'\n",
    "        self.hostname = 'ec2-52-39-242-144.us-west-2.compute.amazonaws.com'\n",
    "        self.url_connect = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=self.hostname, db=self.database_name)\n",
    "        self.properties = {\"user\":\"spark_user\", \n",
    "                      \"password\":os.environ['POSTGRES_PASS'],\n",
    "                      \"driver\": \"org.postgresql.Driver\"\n",
    "                     }\n",
    "    def get_writer(self, df):\n",
    "        return DataFrameWriter(df)\n",
    "        \n",
    "    def write(self, df, table, mode):\n",
    "        my_writer = self.get_writer(df)\n",
    "        my_writer.jdbc(self.url_connect, table, mode, self.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "my_ec2 = 'ec2-52-39-242-144.us-west-2.compute.amazonaws.com'\n",
    "\n",
    "conn = psycopg2.connect(host=my_ec2, database='broadband_scoutDB',\n",
    "                       user='postgres', password='postgres')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Script created from a template found here:\n",
    "# http://www.postgresqltutorial.com/postgresql-python/connect/\n",
    "\n",
    "import psycopg2\n",
    "from config import config\n",
    " \n",
    "def connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # read connection parameters\n",
    "        params = config()\n",
    " \n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params)\n",
    " \n",
    "        # create a cursor\n",
    "        cur = conn.cursor()\n",
    "        \n",
    " # execute a statement\n",
    "        print('PostgreSQL database version:')\n",
    "        cur.execute('SELECT version()')\n",
    " \n",
    "        # display the PostgreSQL database server version\n",
    "        db_version = cur.fetchone()\n",
    "        print(db_version)\n",
    "       \n",
    "     # close the communication with the PostgreSQL\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Script created from template here: \n",
    "# http://www.postgresqltutorial.com/postgresql-python/connect/\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "\n",
    "def config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
