{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some questions I have about my code:\n",
    "\n",
    "- My code mostly uses RDD dataframes. That is, I have code to read files in both SQL Dataframes and RDD Dataframes but all of my code after the \"read\" phase uses the RDD dataframes. However, I'm not sure if it's best/preferable for my purposes to use SQL or RDD dataframes. Is there some good reason I should be choosing RDD over SQL, or vice versa?\n",
    "\n",
    "- On my RDD dataframes, I have written a few different custom functions including (1) setting the correct delimiters, (2) removing unicode information, and (3) a mapping function that preserves only the columns in my datasets that are needed while ignoring those columns in my datasets that are not needed.\n",
    "\n",
    "    - However, what other map/reduce/filter/transformation functions should I be using to prepare my data for my  \n",
    "    - database?\n",
    "\n",
    "\n",
    "#### Towards the end of my code here, I used the following format to write my data back to S3:\n",
    "\n",
    "cleaned_rdd_BROADBAND.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_BROADBAND\")\n",
    "\n",
    "\n",
    "- I have not yet even tested whether this works. However, my questions are \n",
    "\n",
    "    - \"Is this the correct/best method write data back to S3?\"\n",
    "    - \"If not, is there a better method for writing data back to S3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports the needed modules/libraries for reading, transforming, and writing my input data\n",
    "\n",
    "# *****I need to check if some of these import lines are either redundant or otherwise not needed*****\n",
    "\n",
    "import pyspark\n",
    "import spark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"whatever name\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code stops any existing sc -- SparkContext() -- if it is running (because you can't start it if\n",
    "# it's already running) and then restarts it.\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "sc = SparkContext()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: need to fix paths to correct paths in S3 bucket for Broadband, Census, GPS, and other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string object with path and name of the relevant **BROADBAND** dataset available in S3 bucket\n",
    "# This dataset is from data showing available broadband speeds in zip codes, cities, and by othnsus and \n",
    "# includes relevant information for this project like zip codes, census tracts, and other information about \n",
    "# the United States\n",
    "\n",
    "pathname_BROADBAND_dataset = 's3a://sparkforinsightproject/XXXXXBROADBANDXXXXX.txt'\n",
    "\n",
    "\n",
    "# Create string object with path and name of the single most recent **ACQUISITIONS** dataset available in S3 bucket\n",
    "# This dataset contains HOUSING information from the last quarter of 2017 and includes things like housing values \n",
    "# by zip code (and possibly by census tract?, city, state)\n",
    "\n",
    "pathname_ACQUISIT_2017Q4 = 's3a://sparkforinsightproject/fannie_freddie_data/Acquisition_2017Q4.txt'\n",
    "\n",
    "\n",
    "\n",
    "# Create string object with path and name of the single most recent **PERFORMANCE** dataset available in S3 bucket\n",
    "# This dataset contains HOUSING information from the last quarter of 2017 and includes things like housing values \n",
    "# by zip code (and possibly by census tract?, city, state)\n",
    "\n",
    "pathname_PERFORM_2017Q4 = 's3a://sparkforinsightproject/fannie_freddie_data/Performance_2017Q4.txt'\n",
    "\n",
    "\n",
    "\n",
    "# Create string object with path and name of the relevant **CENSUS** dataset available in S3 bucket\n",
    "# This dataset is from the US Census and includes relevant information for this project like zip codes, census \n",
    "# tracts, and other information about the United States\n",
    "\n",
    "\n",
    "pathname_CENSUS_dataset = 's3a://sparkforinsightproject/XXXXXCENSUSXXXXXXX.txt'\n",
    "\n",
    "\n",
    "# Create string object with path and name of the relevant **GPS COORDINATES** dataset available in S3 bucket\n",
    "# This dataset contains GPS COORDINATE information (longitude and latitude) alongside relevant information for\n",
    "# identifying the GPS coordinates in the US paired with with data such as zip codes, city names, states, and \n",
    "# possibly census tracts/counties. Data will be used to calculate the distance of various zip codes from the\n",
    "# city center of cities around the nation, along with the typical/median/average housing prices of those zip\n",
    "# codes / census tracts as well\n",
    "\n",
    "\n",
    "pathname_CENSUS_dataset = 's3a://sparkforinsightproject/XXXXXCENSUSXXXXXXX.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadband data (RDD Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the BROADBAND dataset that shows information about typical broadband speeds in all zip codes/counties, \n",
    "# etc... in the United States. NOTE: RDD is an acronym for Resilient Distributed Dataset (RDD)\n",
    "\n",
    "rdd_BROADBAND = sc.textFile(pathname_BROADBAND_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates view of the top 10 columns in the raw rdd BROADBAND dataframe\n",
    "\n",
    "rdd_BROADBAND.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean/Process/Transform BROADBAND dataset for loading into database\n",
    "# NOTE: The map(lambda x) function is more generally known as a transformation method.\n",
    "# Also note that transformations are lazy operations that return a reference to an RDD object, but Spark\n",
    "# doesn't actually run the transformations at the time of creating this reference. Instead, an action needs \n",
    "# to be peformed to use the RDD reference object, and at that time the action method will run the data.\n",
    "\n",
    "cleaned_rdd_BROADBAND = rdd_BROADBAND.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|')).\\\n",
    "                                                        map(lambda x: x[4], x[8], x[11], x[19], x[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing data (SQL Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in most recent single ACQUISITIONS dataset as SQL Dataframe\n",
    "\n",
    "sql_ACQUISIT_2017Q4 = spark.read.csv(pathname_ACQUISIT_2017Q4, header=False, mode=\"DROPMALFORMED\", encoding='UTF-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in most recent single PERFORMANCE dataset as SQL Dataframe\n",
    "\n",
    "sql_PERFORM_2017Q4 = spark.read.csv(pathname_PERFORM_2017Q4, header=False, mode=\"DROPMALFORMED\", encoding='UTF-8', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing data (RDD Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in most recent single ACQUISITIONS dataset as RDD Dataframe\n",
    "\n",
    "rdd_ACQUISIT_2017Q4 = sc.textFile(pathname_ACQUISIT_2017Q4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates view of the top 10 columns in the raw rdd ACQUISITION RDD # \n",
    "\n",
    "rdd_ACQUISIT_2007Q4.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in most recent single PERFORMANCE dataset as RDD Dataframe\n",
    "\n",
    "rdd_PERFORM_2007q4 = sc.textFile(pathname_PERFORM_2017Q4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates view of the top 10 columns in the raw rdd PERFORMANCE RDD # \n",
    "\n",
    "\n",
    "rdd_PERFORM_2007Q4.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: for Clean/Process/Transform code blocks below, need to fix lambda function -- need to identify which columns are most important/necessary to keep and then slice those columns in the third map(lambda...) command -- eliminating unecessary columns helps to reduce amount of columns needed\n",
    "\n",
    "# Also should convert columns to correct datatypes in this step\n",
    "\n",
    "# Also should consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean/Process/Transform ACQUISITIONS dataset for loading into database\n",
    "\n",
    "cleaned_rdd_ACQUISIT_2017Q4 = rdd_ACQUISIT_2017Q4.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|')).\\\n",
    "                                                        map(lambda x: x[4], x[8], x[11], x[19], x[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Clean/Process/Transform ACQUISITIONS RDD Dataframe for loading into database\n",
    "\n",
    "\n",
    "cleaned_rdd_ACQUISIT_2017Q4 = data_Acquistion_2017Q1.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|')).\\\n",
    "                                                        map(lambda x: x[4], x[8], x[11], x[19], x[34])\n",
    "                                                        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean/Process/Transform PERFORMANCE RDD Dataframe for loading into database\n",
    "\n",
    "\n",
    "cleaned_rdd_PERFORM_2007q4 = rdd_PERFORM_2007q4.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|')).\\\n",
    "                                                        map(lambda x: x[4], x[8], x[11], x[19], x[34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Census data with population, zip codes, census tracts, and other relevant population and geographic data for all parts of the United States\n",
    "\n",
    "rdd_CENSUS = sc.textFile(path_name_CENSUS_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 10 rows of CENSUS RDD\n",
    "\n",
    "rdd_CENSUS.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPS coordinates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GPS coordinates data with GPS coordinates (latitude and longitude), zip codes, and other relevant population and geographic data in the United States\n",
    "\n",
    "rdd_CENSUS = sc.textFile(path_name_GPS_COORDINATES_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 10 rows of CENSUS RDD\n",
    "rdd_GPS_COORDINATE_data.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write all datasets back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write BROADBAND dataset to S3\n",
    "\n",
    "cleaned_rdd_BROADBAND.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_BROADBAND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write HOUSING ACQUISITIONS dataset to S3\n",
    "\n",
    "cleaned_rdd_ACQUISIT2017Q4.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_ACQUISIT2017Q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CENSUS dataset to S3\n",
    "\n",
    "cleaned_rdd_PERFORM2017Q4.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_PERFORM2017Q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CENSUS dataset to S3\n",
    "\n",
    "cleaned_rdd_CENSUS.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_CENSUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write GPS COORDINATES dataset to S3\n",
    "\n",
    "cleaned_rdd_GPS.rdd.repartition(1).saveAsTextFile(\"s3n://sparkforinsightproject/database_data/cleaned_CENSUS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU CAN IGNORE EVERYTHING BELOW THIS POINT \n",
    "# -- IT IS ALL ROUGH/SCRATCH CODE BELOW THIS POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'property' object has no attribute 'csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d5aabbe9b6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m df = SQLContext.read.csv(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbroadband_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DROPMALFORMED\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'property' object has no attribute 'csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Read in the SINGLE, most recent Fannie Mae/Freddie Mac Acquisitions dataset as RDD\n",
    "data_ACQUISIT_2017Q1_rdd = sc.textFile(df_sql_ACQUISIT_2007q4)\n",
    "\n",
    "        \n",
    "# df = SQLContext.read.csv(\n",
    "#     broadband_filename, header=True, mode=\"DROPMALFORMED\"\n",
    "# )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and transform single dataset to be loaded into database\n",
    "\n",
    "data_Acquistion_2017_rdd_cleaned = rdd_ACQUISIT_2017Q4.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Acquistion_2017_rdd_cleaned = data_PERFORM_2017Q4_rdd.map(lambda x: x.encode('ascii', 'ignore')).\\\n",
    "                                                        map(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a LIST of the most recent Fannie Mae/Freddie Mac Acquisitions datasets from each quarter in 2017 that shows housing values by zip code (and possibly by census tract?)\n",
    "\n",
    "for ?? in ???data_Acquistion_2017Q1_rdd = sc.textFile(single_quarter4_acquisitions_dataset)\n",
    "\n",
    "# Read in the most recent Fannie Mae/Freddie Mac Acquisitions dataset from 2017 that shows housing values by zip code (and possibly by census tract?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['objectid',\n",
       "  'random_pt_objectid',\n",
       "  'datasource',\n",
       "  'frn',\n",
       "  'provname',\n",
       "  'dbaname',\n",
       "  'hoconum',\n",
       "  'hoconame',\n",
       "  'stateabbr',\n",
       "  'fullfipsid',\n",
       "  'county_fips',\n",
       "  'transtech',\n",
       "  'maxaddown',\n",
       "  'maxadup',\n",
       "  'typicdown',\n",
       "  'typicup',\n",
       "  'downloadspeed',\n",
       "  'uploadspeed',\n",
       "  'provider_type',\n",
       "  'end_user_cat'],\n",
       " ['16600',\n",
       "  '4837795020014440017',\n",
       "  'RoadSegment',\n",
       "  '0000012781',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'Mountain Zone TV Systems',\n",
       "  '240068',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'TX',\n",
       "  '483779502001444',\n",
       "  '48377',\n",
       "  '41',\n",
       "  '7',\n",
       "  '5',\n",
       "  '5',\n",
       "  '2',\n",
       "  '7',\n",
       "  '5',\n",
       "  '1',\n",
       "  '5'],\n",
       " ['16600',\n",
       "  '4837795020014440018',\n",
       "  'RoadSegment',\n",
       "  '0000012781',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'Mountain Zone TV Systems',\n",
       "  '240068',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'TX',\n",
       "  '483779502001444',\n",
       "  '48377',\n",
       "  '41',\n",
       "  '7',\n",
       "  '5',\n",
       "  '5',\n",
       "  '2',\n",
       "  '7',\n",
       "  '5',\n",
       "  '1',\n",
       "  '5'],\n",
       " ['16685',\n",
       "  '4837795020014440018',\n",
       "  'RoadSegment',\n",
       "  '0000012781',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'Mountain Zone TV Systems',\n",
       "  '240068',\n",
       "  'Neu Ventures, Inc.',\n",
       "  'TX',\n",
       "  '483779502001444',\n",
       "  '48377',\n",
       "  '41',\n",
       "  '7',\n",
       "  '5',\n",
       "  '5',\n",
       "  '2',\n",
       "  '7',\n",
       "  '5',\n",
       "  '1',\n",
       "  '5']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Acquistion_2007_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadband data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'objectid|random_pt_objectid|datasource|frn|provname|dbaname|hoconum|hoconame|stateabbr|fullfipsid|county_fips|transtech|maxaddown|maxadup|typicdown|typicup|downloadspeed|uploadspeed|provider_type|end_user_cat',\n",
       " u'16600|4837795020014440017|RoadSegment|0000012781|Neu Ventures, Inc.|Mountain Zone TV Systems|240068|Neu Ventures, Inc.|TX|483779502001444|48377|41|7|5|5|2|7|5|1|5',\n",
       " u'16600|4837795020014440018|RoadSegment|0000012781|Neu Ventures, Inc.|Mountain Zone TV Systems|240068|Neu Ventures, Inc.|TX|483779502001444|48377|41|7|5|5|2|7|5|1|5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadband_rdd = sc.textFile(broadband_filename)\n",
    "\n",
    "broadband_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64c853f62897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_sql_acquisition_2007q1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://sparkforinsightproject/Acquisition_2007Q1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DROPMALFORMED\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'read'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_acquisition_2007q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_sql_acquisition_2007q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(broadband_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=u'100001461640', _c1=u'R', _c2=u'PNC BANK, N.A.', _c3=u'6.25', _c4=u'137000', _c5=u'360', _c6=u'01/2007', _c7=u'03/2007', _c8=u'56', _c9=u'56', _c10=u'2', _c11=u'37', _c12=u'741', _c13=u'N', _c14=u'C', _c15=u'SF', _c16=u'1', _c17=u'P', _c18=u'MI', _c19=u'486', _c20=None, _c21=u'FRM', _c22=u'734', _c23=None, _c24=u'N'),\n",
       " Row(_c0=u'100015135004', _c1=u'R', _c2=u'SUNTRUST MORTGAGE INC.', _c3=u'6', _c4=u'116000', _c5=u'360', _c6=u'02/2007', _c7=u'04/2007', _c8=u'80', _c9=u'80', _c10=u'2', _c11=u'11', _c12=u'796', _c13=u'N', _c14=u'R', _c15=u'SF', _c16=u'1', _c17=u'S', _c18=u'GA', _c19=u'302', _c20=None, _c21=u'FRM', _c22=u'762', _c23=None, _c24=u'N'),\n",
       " Row(_c0=u'100015306566', _c1=u'C', _c2=u'CITIMORTGAGE, INC.', _c3=u'6.375', _c4=u'58000', _c5=u'180', _c6=u'02/2007', _c7=u'03/2007', _c8=u'78', _c9=u'78', _c10=u'2', _c11=u'30', _c12=u'710', _c13=u'N', _c14=u'R', _c15=u'SF', _c16=u'1', _c17=u'P', _c18=u'IN', _c19=u'465', _c20=None, _c21=u'FRM', _c22=None, _c23=None, _c24=u'N'),\n",
       " Row(_c0=u'100015319835', _c1=u'C', _c2=u'BANK OF AMERICA, N.A.', _c3=u'6.125', _c4=u'353000', _c5=u'360', _c6=u'12/2006', _c7=u'02/2007', _c8=u'80', _c9=u'80', _c10=u'2', _c11=u'28', _c12=u'778', _c13=u'N', _c14=u'R', _c15=u'SF', _c16=u'1', _c17=u'P', _c18=u'MA', _c19=u'021', _c20=None, _c21=u'FRM', _c22=u'656', _c23=None, _c24=u'N'),\n",
       " Row(_c0=u'100030521552', _c1=u'C', _c2=u'GMAC MORTGAGE, LLC', _c3=u'5.875', _c4=u'385000', _c5=u'360', _c6=u'12/2006', _c7=u'03/2007', _c8=u'70', _c9=u'70', _c10=u'2', _c11=u'50', _c12=u'720', _c13=u'N', _c14=u'C', _c15=u'SF', _c16=u'1', _c17=u'P', _c18=u'CA', _c19=u'917', _c20=None, _c21=u'FRM', _c22=u'700', _c23=None, _c24=u'N')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_acquisition_2007q1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-2ae4886beff9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-2ae4886beff9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data_Acquistion_2007_rdd = sc.textFile(filename).map(lamdba x: x.split('|'))\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_Acquistion_2007_rdd = sc.textFile(filename).map(lamdba x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.map(lambda x: x.split('\\t')).\\\n",
    "    map(lambda y: (y[0], y[2], y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "subtract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-67082cab7e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbroadband_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadband_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbroadband_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbroadband_first\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbroadband_w0_header_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadband_first\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadband_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: subtract"
     ]
    }
   ],
   "source": [
    "broadband_first = broadband_rdd.first()\n",
    "broadband_header = sc.parallelize([broadband_first])\n",
    "broadband_w0_header_rdd = broadband_first.subtract(broadband_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadband_xxx = broadband_rdd.map(lambda x: x.split('|')).map(lambda x: x.encode('ascii', 'ignore'))\n",
    "\n",
    "broadband_rdd = spark.read.option(\"header\",\"true\").csv(broadband_filename)\n",
    "\n",
    "# header = broadband_rdd.first() #extract header\n",
    "# broadband_rdd = broadband_rdd.filter(row => row != header) \n",
    "\n",
    "broadband_xxx = broadband_rdd.map(lambda x: x.encode('ascii', 'ignore').\\\n",
    "                                  split('|')).\\\n",
    "                                 map(lambda y: (y[0], y[1], y[2], y[3], y[4], y[5], y[6], y[7], y[8], y[9], y[10], y[11], y[12], y[13], y[14], y[15], y[16], y[17], y[18], y[19]))\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-130e883d5712>\", line 3, in <lambda>\nValueError: invalid literal for int() with base 10: 'objectid'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-130e883d5712>\", line 3, in <lambda>\nValueError: invalid literal for int() with base 10: 'objectid'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ca6db6aa5695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbroadband_xxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-130e883d5712>\", line 3, in <lambda>\nValueError: invalid literal for int() with base 10: 'objectid'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-130e883d5712>\", line 3, in <lambda>\nValueError: invalid literal for int() with base 10: 'objectid'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "broadband_xxx.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_Acquistion_2007_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'100001461640|R|PNC BANK, N.A.|6.25|137000|360|01/2007|03/2007|56|56|2|37|741|N|C|SF|1|P|MI|486||FRM|734||N',\n",
       " u'100015135004|R|SUNTRUST MORTGAGE INC.|6|116000|360|02/2007|04/2007|80|80|2|11|796|N|R|SF|1|S|GA|302||FRM|762||N',\n",
       " u'100015306566|C|CITIMORTGAGE, INC.|6.375|58000|180|02/2007|03/2007|78|78|2|30|710|N|R|SF|1|P|IN|465||FRM|||N',\n",
       " u'100015319835|C|BANK OF AMERICA, N.A.|6.125|353000|360|12/2006|02/2007|80|80|2|28|778|N|R|SF|1|P|MA|021||FRM|656||N',\n",
       " u'100030521552|C|GMAC MORTGAGE, LLC|5.875|385000|360|12/2006|03/2007|70|70|2|50|720|N|C|SF|1|P|CA|917||FRM|700||N']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Acquistion_2007_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sc.textFile(filename)\\\n",
    "... .map(lambda x: x.encode('ascii', 'ignore').split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1\n",
    "\n",
    "def toCSVLine(data):\n",
    "  return ','.join(str(d) for d in data)\n",
    "\n",
    "lines = labelsAndPredictions.map(toCSVLine)\n",
    "lines.saveAsTextFile('hdfs://my-node:9000/tmp/labels-and-predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100001461640',\n",
       "  'R',\n",
       "  'PNC BANK, N.A.',\n",
       "  '6.25',\n",
       "  '137000',\n",
       "  '360',\n",
       "  '01/2007',\n",
       "  '03/2007',\n",
       "  '56',\n",
       "  '56',\n",
       "  '2',\n",
       "  '37',\n",
       "  '741',\n",
       "  'N',\n",
       "  'C',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'MI',\n",
       "  '486',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '734',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100015135004',\n",
       "  'R',\n",
       "  'SUNTRUST MORTGAGE INC.',\n",
       "  '6',\n",
       "  '116000',\n",
       "  '360',\n",
       "  '02/2007',\n",
       "  '04/2007',\n",
       "  '80',\n",
       "  '80',\n",
       "  '2',\n",
       "  '11',\n",
       "  '796',\n",
       "  'N',\n",
       "  'R',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'S',\n",
       "  'GA',\n",
       "  '302',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '762',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100015306566',\n",
       "  'C',\n",
       "  'CITIMORTGAGE, INC.',\n",
       "  '6.375',\n",
       "  '58000',\n",
       "  '180',\n",
       "  '02/2007',\n",
       "  '03/2007',\n",
       "  '78',\n",
       "  '78',\n",
       "  '2',\n",
       "  '30',\n",
       "  '710',\n",
       "  'N',\n",
       "  'R',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'IN',\n",
       "  '465',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100015319835',\n",
       "  'C',\n",
       "  'BANK OF AMERICA, N.A.',\n",
       "  '6.125',\n",
       "  '353000',\n",
       "  '360',\n",
       "  '12/2006',\n",
       "  '02/2007',\n",
       "  '80',\n",
       "  '80',\n",
       "  '2',\n",
       "  '28',\n",
       "  '778',\n",
       "  'N',\n",
       "  'R',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'MA',\n",
       "  '021',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '656',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100030521552',\n",
       "  'C',\n",
       "  'GMAC MORTGAGE, LLC',\n",
       "  '5.875',\n",
       "  '385000',\n",
       "  '360',\n",
       "  '12/2006',\n",
       "  '03/2007',\n",
       "  '70',\n",
       "  '70',\n",
       "  '2',\n",
       "  '50',\n",
       "  '720',\n",
       "  'N',\n",
       "  'C',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'CA',\n",
       "  '917',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '700',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100034251351',\n",
       "  'R',\n",
       "  'OTHER',\n",
       "  '6.25',\n",
       "  '117000',\n",
       "  '360',\n",
       "  '03/2007',\n",
       "  '05/2007',\n",
       "  '90',\n",
       "  '90',\n",
       "  '2',\n",
       "  '41',\n",
       "  '753',\n",
       "  'N',\n",
       "  'P',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'OH',\n",
       "  '449',\n",
       "  '17',\n",
       "  'FRM',\n",
       "  '743',\n",
       "  '1',\n",
       "  'N'],\n",
       " ['100036401006',\n",
       "  'R',\n",
       "  'OTHER',\n",
       "  '5.75',\n",
       "  '200000',\n",
       "  '360',\n",
       "  '01/2007',\n",
       "  '03/2007',\n",
       "  '46',\n",
       "  '46',\n",
       "  '2',\n",
       "  '25',\n",
       "  '674',\n",
       "  'N',\n",
       "  'C',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'CT',\n",
       "  '060',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '763',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100037298334',\n",
       "  'C',\n",
       "  'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION',\n",
       "  '6.375',\n",
       "  '219000',\n",
       "  '360',\n",
       "  '12/2006',\n",
       "  '02/2007',\n",
       "  '80',\n",
       "  '92',\n",
       "  '2',\n",
       "  '27',\n",
       "  '749',\n",
       "  'Y',\n",
       "  'P',\n",
       "  'PU',\n",
       "  '1',\n",
       "  'P',\n",
       "  'NC',\n",
       "  '282',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100039008927',\n",
       "  'C',\n",
       "  'CITIMORTGAGE, INC.',\n",
       "  '6.375',\n",
       "  '198000',\n",
       "  '360',\n",
       "  '12/2006',\n",
       "  '02/2007',\n",
       "  '54',\n",
       "  '54',\n",
       "  '1',\n",
       "  '13',\n",
       "  '777',\n",
       "  'N',\n",
       "  'C',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'S',\n",
       "  'CA',\n",
       "  '961',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '',\n",
       "  '',\n",
       "  'N'],\n",
       " ['100048042494',\n",
       "  'R',\n",
       "  'OTHER',\n",
       "  '5.875',\n",
       "  '55000',\n",
       "  '180',\n",
       "  '03/2007',\n",
       "  '05/2007',\n",
       "  '37',\n",
       "  '37',\n",
       "  '1',\n",
       "  '41',\n",
       "  '619',\n",
       "  'N',\n",
       "  'C',\n",
       "  'SF',\n",
       "  '1',\n",
       "  'P',\n",
       "  'AR',\n",
       "  '727',\n",
       "  '',\n",
       "  'FRM',\n",
       "  '',\n",
       "  '',\n",
       "  'N']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253292"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5586c02e305d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m253292\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "for i in df1.take(253292):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
